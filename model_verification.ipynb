{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ab48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from model_components import FM, LinearPart, DNN, DeepFM\n",
    "from deepctr_torch.layers.core import DNN as DNN_deepctr\n",
    "from utils import seed_everything\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, build_input_features\n",
    "from deepctr_torch.models.deepfm import DeepFM as OfficialDeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb26b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시드 고정\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362cf8e",
   "metadata": {},
   "source": [
    "1. FM 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bc1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_stack shape : torch.Size([2, 3, 4])\n",
      "FM output         : tensor([2.2455, 0.4180])\n",
      "Manual output     : tensor([2.2455, 0.4180])\n"
     ]
    }
   ],
   "source": [
    "embed_stack = torch.randn(2, 3, 4, dtype=torch.float32)\n",
    "\n",
    "fm_layer = FM()\n",
    "fm_out = fm_layer(embed_stack)\n",
    "\n",
    "# deepctr의 FM과 원본 식 비교\n",
    "square_of_sum = torch.pow(torch.sum(embed_stack, dim=1, keepdim=True), 2)\n",
    "sum_of_square = torch.sum(embed_stack * embed_stack, dim=1, keepdim=True)\n",
    "cross_term = square_of_sum - sum_of_square\n",
    "cross_term = 0.5 * torch.sum(cross_term, dim=2, keepdim=False)\n",
    "\n",
    "assert torch.allclose(fm_out, cross_term, atol=1e-6), \"불일치!\"\n",
    "\n",
    "print(\"embed_stack shape :\", embed_stack.shape)\n",
    "print(\"FM output :\", fm_out.squeeze(-1).squeeze(-1))\n",
    "print(\"Manual output :\", cross_term.squeeze(-1).squeeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00352a93",
   "metadata": {},
   "source": [
    "2. DNN 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ed0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepctr output : tensor([[0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "custom  output : tensor([[0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000],\n",
      "        [0.0787, 0.0000, 0.1409, 0.0000, 0.0006, 0.1589, 0.1758, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "max |diff|     : 0.0\n"
     ]
    }
   ],
   "source": [
    "input_dim = 16\n",
    "hidden_units = (32, 8)\n",
    "activation = \"relu\"\n",
    "dropout_rate = 0.0\n",
    "use_bn = False\n",
    "device = \"cpu\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dnn_dc = DNN_deepctr(\n",
    "    input_dim,\n",
    "    hidden_units=hidden_units,\n",
    "    activation=activation,\n",
    "    dropout_rate=dropout_rate,\n",
    "    use_bn=use_bn,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "dnn_my = DNN(\n",
    "    input_dim,\n",
    "    hidden_units=hidden_units,\n",
    "    activation=activation,\n",
    "    dropout_rate=dropout_rate,\n",
    "    use_bn=use_bn,\n",
    ").to(device)\n",
    "\n",
    "for src, tgt in zip(dnn_dc.linears, dnn_my.linears):\n",
    "    tgt.weight.data.copy_(src.weight.data)\n",
    "    tgt.bias.data.copy_(src.bias.data)\n",
    "\n",
    "if use_bn:\n",
    "    for src, tgt in zip(dnn_dc.bn, dnn_my.bn):\n",
    "        for attr in [\"weight\", \"bias\", \"running_mean\", \"running_var\"]:\n",
    "            getattr(tgt, attr).data.copy_(getattr(src, attr).data)\n",
    "\n",
    "X = torch.randn(5, input_dim, device=device)   # batch=5\n",
    "\n",
    "out_dc = dnn_dc(X)\n",
    "out_my = dnn_my(X)\n",
    "\n",
    "print(\"deepctr output :\", out_dc)\n",
    "print(\"custom  output :\", out_my)\n",
    "print(\"max |diff|     :\", (out_dc - out_my).abs().max().item())\n",
    "\n",
    "assert torch.allclose(out_dc, out_my, atol=1e-6, rtol=1e-6), \"불일치!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cadad",
   "metadata": {},
   "source": [
    "3. Linear 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom LinearPart 출력:\n",
      " [[-4.3112147e-04]\n",
      " [-1.7197993e-04]\n",
      " [-1.7077866e-05]\n",
      " [-7.7292119e-05]]\n",
      "Official linear_model 출력:\n",
      " [[-4.3112147e-04]\n",
      " [-1.7197993e-04]\n",
      " [-1.7077866e-05]\n",
      " [-7.7292119e-05]]\n",
      "최대 절대 오차: 0.0\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "    SparseFeat('feat1', vocabulary_size=10, embedding_dim=4),\n",
    "    SparseFeat('feat2', vocabulary_size=5, embedding_dim=4),\n",
    "    DenseFeat ('feat3', dimension=1)\n",
    "]\n",
    "\n",
    "linear_part = LinearPart(feature_columns, device='cpu')\n",
    "official_model = OfficialDeepFM(\n",
    "    linear_feature_columns=feature_columns,\n",
    "    dnn_feature_columns=[],       # DNN 없이 linear 파트만 사용\n",
    "    l2_reg_linear=0,\n",
    "    l2_reg_embedding=0,\n",
    "    init_std=0.0001,\n",
    "    seed=1024,\n",
    "    task='binary'\n",
    ")\n",
    "\n",
    "# 공식 linear_model의 가중치 복사\n",
    "for fc in feature_columns:\n",
    "    if isinstance(fc, SparseFeat):\n",
    "        name = fc.embedding_name  # 보통 name과 동일\n",
    "        emb_weight = official_model.linear_model.embedding_dict[name].weight.data\n",
    "        linear_part.sparse_linears[name].weight.data.copy_(emb_weight)\n",
    "\n",
    "if hasattr(official_model.linear_model, 'weight'):\n",
    "    w = official_model.linear_model.weight.data  # shape: (dense_dim,1)\n",
    "    linear_part.dense_linear.weight.data.copy_(w.t())\n",
    "\n",
    "feature_index = build_input_features(feature_columns)\n",
    "dense_idx = [feature_index[fc.name][0] for fc in feature_columns if isinstance(fc, DenseFeat)]\n",
    "\n",
    "batch_size = 4\n",
    "num_features = len(feature_index)\n",
    "X_np = np.zeros((batch_size, num_features), dtype=np.float32)\n",
    "\n",
    "for fc in feature_columns:\n",
    "    idx = feature_index[fc.name][0]\n",
    "    if isinstance(fc, SparseFeat):\n",
    "        X_np[:, idx] = np.random.randint(0, fc.vocabulary_size, size=batch_size)\n",
    "    else:\n",
    "        X_np[:, idx] = np.random.randn(batch_size)\n",
    "X = torch.from_numpy(X_np)\n",
    "\n",
    "out_custom = linear_part(X, feature_index, dense_idx)\n",
    "out_official = official_model.linear_model(X)\n",
    "\n",
    "print(\"Custom LinearPart 출력:\\n\", out_custom.detach().numpy())\n",
    "print(\"Official linear_model 출력:\\n\", out_official.detach().numpy())\n",
    "print(\"최대 절대 오차:\", float((out_custom - out_official).abs().max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4a229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
